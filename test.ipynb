{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed9ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 26 16:38:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   35C    P8             23W /  300W |       1MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903118f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16008b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = os.environ.get(\"MODEL_ID\", \"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "DTYPE = torch.bfloat16\n",
    "DEVICE_MAP = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14bacf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8937f3039359453582fc9191ad73ccbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc88176a75ab4cfcb21990768f716a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6d1af541d74b439242040a103408d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2794e7bc6864457083021d26829e317a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e417fb75346b495e943c8651257c09a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78a0450a2b04b0a80934bf719159dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b949d6842c4334bd701fe2d167e89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c1d042892b4bf5addfcca24a58de7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d890d1a7d84ef1a3c9c3bbab893915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c704ca2bbbb4fbba9d7c801b8c8afe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e5b9db57754eab8a8a4757b26adb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af0ee4ed9434328868aa60625c1984f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
      "  \"latency_s\": 9.9482,\n",
      "  \"tokens_generated\": 256,\n",
      "  \"throughput_tok_per_s\": 25.73,\n",
      "  \"peak_gpu_mem_gb\": 13.55\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"You are a helpful assistant. Briefly explain attention and the KV cache in LLMs.\"\"\"\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=DEVICE_MAP,\n",
    "    use_cache=True,                 # full KV-cache (baseline)\n",
    "    attn_implementation=\"eager\",    # standard attention\n",
    ")\n",
    "\n",
    "inputs = tok(PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Simple latency measurement\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "    )\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "text = tok.decode(out[0], skip_special_tokens=True)\n",
    "latency_s = t1 - t0\n",
    "toks_out = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "tps = toks_out / latency_s if latency_s > 0 else float(\"nan\")\n",
    "peak_mem = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "\n",
    "result = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"latency_s\": round(latency_s, 4),\n",
    "    \"tokens_generated\": int(toks_out),\n",
    "    \"throughput_tok_per_s\": round(tps, 2),\n",
    "    \"peak_gpu_mem_gb\": round(peak_mem, 2),\n",
    "}\n",
    "\n",
    "print(json.dumps(result, indent=2))\n",
    "logdir = os.environ.get(\"LOGDIR\", f\"{os.environ.get('SCRATCH','.')}/logs\")\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "with open(os.path.join(logdir, \"baseline_generate.json\"), \"w\") as f:\n",
    "    json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10376178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
