_wandb:
    value:
        cli_version: 0.23.1
        e:
            5ipbd78wps0ghg2eu8io19sr9vkml86h:
                codePath: narrativeqa_eval.py
                codePathLocal: narrativeqa_eval.py
                cpu_count: 96
                cpu_count_logical: 192
                cudaVersion: "12.9"
                disk:
                    /:
                        total: "74968989696"
                        used: "44292034560"
                email: bk2951@columbia.edu
                executable: /insomnia001/home/bk2951/hpml-longctx-optim/.venv/bin/python
                git:
                    commit: 5c7c75e891d0fe73eb98b38bf7aa83e696282556
                    remote: https://github.com/brandonkeung/hpml-longctx-optim.git
                gpu: NVIDIA RTX A6000
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-114f5efb-87c0-5c82-017f-f1a7d2f28cb5
                host: ins088
                memory:
                    total: "1081451167744"
                os: Linux-5.14.0-362.8.1.el9_3.x86_64-x86_64-with-glibc2.34
                program: /insomnia001/home/bk2951/hpml-longctx-optim/narrativeqa_eval.py
                python: CPython 3.9.18
                root: /insomnia001/home/bk2951/hpml-longctx-optim
                slurm:
                    cluster_name: insomnia
                    conf: /etc/slurm/slurm.conf
                    cpus_on_node: "2"
                    gpus_on_node: "1"
                    gtids: "0"
                    job_account: edu
                    job_cpus_per_node: "2"
                    job_end_time: "1765258291"
                    job_gid: "500"
                    job_group: user
                    job_id: "5666677"
                    job_name: bash
                    job_nodelist: ins088
                    job_num_nodes: "1"
                    job_partition: short
                    job_qos: interactive
                    job_start_time: "1765251091"
                    job_uid: "560037"
                    job_user: bk2951
                    jobid: "5666677"
                    launch_node_ipaddr: 10.197.95.250
                    localid: "0"
                    nnodes: "1"
                    nodeid: "0"
                    nodelist: ins088
                    nprocs: "1"
                    ntasks: "1"
                    oom_kill_step: "0"
                    prio_process: "0"
                    procid: "0"
                    pty_port: "35293"
                    pty_win_col: "118"
                    pty_win_row: "14"
                    srun_comm_host: 10.197.95.250
                    srun_comm_port: "44291"
                    step_gpus: "0"
                    step_id: "0"
                    step_launcher_port: "44291"
                    step_nodelist: ins088
                    step_num_nodes: "1"
                    step_num_tasks: "1"
                    step_tasks_per_node: "1"
                    stepid: "0"
                    submit_dir: /insomnia001/home/bk2951/hpml-longctx-optim
                    submit_host: 2402-login-002
                    task_pid: "1559185"
                    tasks_per_node: "1"
                    topology_addr: ins088
                    topology_addr_pattern: node
                    umask: "0027"
                startedAt: "2025-12-09T04:02:41.035922Z"
                writerId: 5ipbd78wps0ghg2eu8io19sr9vkml86h
        m: []
        python_version: 3.9.18
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 71
                - 100
            "2":
                - 1
                - 11
                - 49
                - 51
                - 71
                - 100
            "3":
                - 2
                - 13
                - 16
            "4": 3.9.18
            "5": 0.23.1
            "6": 4.57.3
            "12": 0.23.1
            "13": linux-x86_64
attn_impl:
    value: flash2
batch_size:
    value: 1
contexts:
    value:
        - 4096
        - 8192
device_map:
    value: auto
dtype:
    value: torch.bfloat16
keep_ratio:
    value: 0.7
kv_mode:
    value: l2
max_new_tokens:
    value: 64
model_id:
    value: mistralai/Mistral-7B-Instruct-v0.3
n_samples:
    value: 50
prune_after:
    value: 512
skip_layers:
    value:
        - 0
        - 1
use_4bit:
    value: false
