_wandb:
    value:
        cli_version: 0.23.1
        e:
            era8h9y07zztrrcrzhnwygxualy1nmep:
                codePath: narrativeqa_eval.py
                codePathLocal: narrativeqa_eval.py
                cpu_count: 96
                cpu_count_logical: 192
                cudaVersion: "12.9"
                disk:
                    /:
                        total: "923746840576"
                        used: "31091937280"
                email: bk2951@columbia.edu
                executable: /insomnia001/home/bk2951/hpml-longctx-optim/.venv/bin/python
                git:
                    commit: f80ffa7b58df10d063e148e590bc5adc31d3d184
                    remote: https://github.com/brandonkeung/hpml-longctx-optim.git
                gpu: NVIDIA RTX A6000
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-f1e25854-6046-617b-48bf-e79ab4b336b9
                host: ins084
                memory:
                    total: "1081440198656"
                os: Linux-5.14.0-570.12.1.el9_6.x86_64-x86_64-with-glibc2.34
                program: /insomnia001/home/bk2951/hpml-longctx-optim/narrativeqa_eval.py
                python: CPython 3.9.21
                root: /insomnia001/home/bk2951/hpml-longctx-optim
                slurm:
                    cluster_name: insomnia
                    conf: /etc/slurm/slurm.conf
                    cpus_on_node: "2"
                    gpus_on_node: "1"
                    gtids: "0"
                    job_account: edu
                    job_cpus_per_node: "2"
                    job_end_time: "1765423272"
                    job_gid: "500"
                    job_group: user
                    job_id: "5730925"
                    job_name: bash
                    job_nodelist: ins084
                    job_num_nodes: "1"
                    job_partition: short
                    job_qos: interactive
                    job_start_time: "1765416072"
                    job_uid: "560037"
                    job_user: bk2951
                    jobid: "5730925"
                    launch_node_ipaddr: 10.197.95.250
                    localid: "0"
                    nnodes: "1"
                    nodeid: "0"
                    nodelist: ins084
                    nprocs: "1"
                    ntasks: "1"
                    oom_kill_step: "0"
                    prio_process: "0"
                    procid: "0"
                    pty_port: "44565"
                    pty_win_col: "108"
                    pty_win_row: "16"
                    srun_comm_host: 10.197.95.250
                    srun_comm_port: "35207"
                    step_gpus: "0"
                    step_id: "0"
                    step_launcher_port: "35207"
                    step_nodelist: ins084
                    step_num_nodes: "1"
                    step_num_tasks: "1"
                    step_tasks_per_node: "1"
                    stepid: "0"
                    submit_dir: /insomnia001/home/bk2951/hpml-longctx-optim
                    submit_host: 2402-login-002
                    task_pid: "1256504"
                    tasks_per_node: "1"
                    topology_addr: ins084
                    topology_addr_pattern: node
                    umask: "0027"
                startedAt: "2025-12-11T01:39:43.711516Z"
                writerId: era8h9y07zztrrcrzhnwygxualy1nmep
        m: []
        python_version: 3.9.21
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 71
                - 100
            "2":
                - 1
                - 11
                - 49
                - 51
                - 71
                - 100
            "3":
                - 2
                - 13
                - 16
            "4": 3.9.21
            "5": 0.23.1
            "6": 4.57.3
            "12": 0.23.1
            "13": linux-x86_64
attn_impl:
    value: sdpa
batch_size:
    value: 1
contexts:
    value:
        - 2048
        - 4096
        - 8192
        - 16384
device_map:
    value: auto
dtype:
    value: torch.bfloat16
keep_ratio:
    value: 0.7
kv_mode:
    value: none
max_new_tokens:
    value: 64
model_id:
    value: Qwen/Qwen1.5-1.8B-Chat
n_samples:
    value: 50
prune_after:
    value: 512
skip_layers:
    value:
        - 0
        - 1
use_4bit:
    value: false
