{
  "run_meta": {
    "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
    "model": "mistralai/Mistral-7B-Instruct-v0.3",
    "attn": "flash2",
    "dtype": "torch.bfloat16",
    "device_map": "auto",
    "contexts": [
      16384,
      32768
    ],
    "n_samples": 50,
    "batch_size": 8,
    "max_new_tokens": 64,
    "timestamp": "20251027-183945"
  },
  "ctx_summaries": [
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 16384,
      "n_requests": 7,
      "latency_ms_p50": 9338.55,
      "latency_ms_p95": 9338.55,
      "ms_per_token_p50": 145.91,
      "ms_per_token_p95": 145.91,
      "tok_per_s_p50": 6.85,
      "tok_per_s_p95": 6.85,
      "peak_gpu_mem_gb_p95": 21.15
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 32768,
      "n_requests": 7,
      "latency_ms_p50": 18810.43,
      "latency_ms_p95": 18810.43,
      "ms_per_token_p50": 293.91,
      "ms_per_token_p95": 293.91,
      "tok_per_s_p50": 3.4,
      "tok_per_s_p95": 3.4,
      "peak_gpu_mem_gb_p95": 28.79
    }
  ],
  "rows": [
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 16384,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 47.40 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 16384,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 47.40 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 16384,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 47.40 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 16384,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 47.40 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 16384,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 47.40 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 16384,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 47.40 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 16384,
      "batch_size": 2,
      "latency_ms_total": 9338.55,
      "avg_gen_tokens": 64.0,
      "tok_per_s": 6.85,
      "ms_per_token": 145.91,
      "peak_gpu_mem_gb": 21.15,
      "success": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 32768,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 47.40 GiB of which 4.06 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 32768,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 47.40 GiB of which 4.06 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 32768,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 47.40 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 32768,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 47.40 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 32768,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 47.40 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 32768,
      "batch_size": 8,
      "error": "CUDA out of memory. Tried to allocate 7.00 GiB. GPU 0 has a total capacity of 47.40 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process ha",
      "success": false,
      "oom": true
    },
    {
      "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "attn": "flash2",
      "context_tokens": 32768,
      "batch_size": 2,
      "latency_ms_total": 18810.43,
      "avg_gen_tokens": 64.0,
      "tok_per_s": 3.4,
      "ms_per_token": 293.91,
      "peak_gpu_mem_gb": 28.79,
      "success": true
    }
  ],
  "summary": {
    "run_id": "Mistral-7B-Instruct-v0.3_flash2_N50_B8_20251027-183945",
    "model": "mistralai/Mistral-7B-Instruct-v0.3",
    "attn": "flash2",
    "total_samples": 50,
    "batch_size": 8,
    "contexts": [
      16384,
      32768
    ],
    "oom_count": 12,
    "EM_rough": 0.04,
    "timestamp": "20251027-183945"
  }
}