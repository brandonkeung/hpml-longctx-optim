# hpml-longctx-optim
Optimizing long-context LLM inference by integrating Flash/FlexAttention and L2 KV-cache compression. Benchmarks up to 32k tokens on latency, throughput, memory usage, and task quality to demonstrate faster, lighter, and scalable inference.
